{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c6622e2-24df-40eb-ad1b-01823c25f803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyyaml (yaml) :: parses configuration files (YAML files)\n",
    "# see https://www.cloudbees.com/blog/yaml-tutorial-everything-you-need-get-started for more information on YAML files\n",
    "import yaml \n",
    "\n",
    "# huggingface :: datasets : dataset-handling libraries from huggingface\n",
    "from datasets import load_dataset\n",
    "#from datasets.filesystems import S3FileSystem # for S3 interactions\n",
    "\n",
    "# huggingface :: transformers : transformer, trainer and tokenizer objects for the actual training\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "# transformer_imports.py :: contains all our transformer imports and the MODEL DICT\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# pytorch (torch) :: machine learning and deep learning method library\n",
    "import torch\n",
    "\n",
    "# gradient checkpointing to fit larger models (when not even 1 batch fits)\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "# nvidia management library, interface with gpu like nvidia-smi\n",
    "from pynvml import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4f615c9-3e86-4566-8957-382ad72fd41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import nvidia_smi\n",
    "import wandb\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "torch.manual_seed(42)\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf0a2179-bd3d-4d51-9db8-283af5318a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "nvidia_smi.nvmlInit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "027eaab0-abc3-4cec-b375-d5bdf2245067",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk punkt sentence tokenizer, divides text into a list of sentences\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bef5de79-8cc4-4b63-b5d5-a2bb874ea8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr  3 20:59:59 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   37C    P8    14W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ab442ab-2313-48a6-9f38-de884ff4c1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a317b65b-5fb2-4005-9c31-412332bb6b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39ab29e4-3481-4eb7-9170-bfb13c36509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_config = \"../configs/gpt2-refl-29-mar-2023.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59a9d478-d8e4-45c9-b8e0-51be9d07d66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open yaml config as a strema and load into config_dict\n",
    "with open(path_to_config, \"r\") as stream:\n",
    "    try:\n",
    "        config_dict = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(\"Configuration load failed!\")\n",
    "        print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "607b596e-762a-4735-a413-00f094c030b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(config_dict[\"data_train_path\"])\n",
    "df_val = pd.read_csv(config_dict[\"data_validation_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d46f7a90-2ddc-4d32-9b54-de380dcfb35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)  # drop NA values\n",
    "triplets = df.triplet.copy()  # copy over triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc0eea85-8da6-4337-8aca-e8945c78c2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_triplets = df_val.triplet.copy()  # validation triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "839c2a2e-7c5c-4031-9a15-a929c4e3cd94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7fcd28aa6f90>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# how long is our training data?\n",
    "doc_lengths = []\n",
    "for triplet in triplets:\n",
    "    tokens = nltk.word_tokenize(triplet)\n",
    "    doc_lengths.append(len(tokens))\n",
    "doc_lengths = np.asarray(doc_lengths)\n",
    "sns.displot(doc_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7be28cbd-b469-486c-94a1-1b8fb23540cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47.53465346534654"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(doc_lengths)\n",
    "# on average, we have ~47.5 tokens per entry, a good thing for GPT2 embedding size of 768 in gpt-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5bad70c-7c11-4cfb-9ad1-612444b2265c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = config_dict['model_name']\n",
    "hyperparameters = config_dict['training_settings']['hyperparameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "faf1d5f9-098e-42b0-833b-4fdd38d29af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt2-medium'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04be7ade-4c30-415d-b9c9-c4efe4dc6b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load gpt-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be6d3e9a-1e9b-44bb-a0bc-230a0c17dde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2bdd305-2a1e-43bf-85f4-c1379d111765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max model length is 1024 for this model, although the actual embedding size for GPT small is 768\n",
      "The beginning of sequence token <|endoftext|> token has the id 50256\n",
      "The end of sequence token <|endoftext|> has the id 50256\n",
      "The padding token <|endoftext|> has the id 50256\n"
     ]
    }
   ],
   "source": [
    "print(\"The max model length is {} for this model, although the actual embedding size for GPT small is 768\".format(tokenizer.model_max_length))\n",
    "print(\"The beginning of sequence token {} token has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.bos_token_id), tokenizer.bos_token_id))\n",
    "print(\"The end of sequence token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.eos_token_id), tokenizer.eos_token_id))\n",
    "print(\"The padding token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.pad_token_id), tokenizer.pad_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e90c3273-a64c-4b7f-b3e8-6e09d47e1256",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-9f66f072da4e52b1\n",
      "Found cached dataset csv (/home/ubuntu/.cache/huggingface/datasets/csv/default-9f66f072da4e52b1/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 662.40it/s]\n",
      "Using custom data configuration default-c257cc22185509a4\n",
      "Found cached dataset csv (/home/ubuntu/.cache/huggingface/datasets/csv/default-c257cc22185509a4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 849.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['triplet'],\n",
      "        num_rows: 808\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['triplet'],\n",
      "        num_rows: 95\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = load_dataset('csv', data_files=config_dict['data_train_path'])\n",
    "raw_datasets[\"validation\"] = (load_dataset('csv', data_files=config_dict['data_validation_path']))[\"train\"]\n",
    "print(raw_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de38c28e-9099-4d33-8fda-1a674e8c6f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIPLET: Prompt: Now, what is the thing you like least about smoking?\n",
      "Response: The medical risks\n",
      "Reflection: You are aware of the medical risks of smoking and you worry about what might happen if you do not quit.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in raw_datasets[\"train\"][0]:\n",
    "    print(f\"{key.upper()}: {raw_datasets['train'][0][key][:256]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f7830ce-a4e0-4f27-a75e-3074b63d2eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper for tokenizing everything\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"triplet\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1d7a71e-23d9-4c88-93b6-76ba3fded68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/csv/default-9f66f072da4e52b1/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1892bd87a0f2b54e.arrow\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 18.33ba/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00dc62af-2a11-43ff-9280-0fc38d1eeaa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['triplet', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 808\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['triplet', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 95\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7915517-e950-4c88-8eca-fcdc75b68efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns([\"triplet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "074cb295-abc7-404d-9674-2f751e9863b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# amount of examples passed through model before a backwards pass\n",
    "batch_size = hyperparameters['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab14844a-0bee-425d-9036-8b9e93c43f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config to instantiate model\n",
    "configuration = GPT2Config.from_pretrained(model_name, output_hidden_states=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f2b34ca-76bd-4c8d-a2fa-820003e6efba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name, config=configuration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ffedcee-b6c7-43d8-b622-68527843efb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50257, 1024)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resize token embeddings for our custom tokens (e.g. bos_token)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b9dd3b69-eaea-4e7e-8de3-1d58a81ec9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ae74060-8628-4706-b597-14b2d8597a31",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (12): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (13): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (14): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (15): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (16): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (17): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (18): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (19): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (20): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (21): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (22): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (23): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sends model to current device - in this case CUDA\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f5dc18a1-03e3-4c02-8d84-6e05af269bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed value\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1d95614b-42d4-4ad2-9034-77929bbe71b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# all the below set the corresponding values from the configuration file config.yaml:\n",
    "model_name = config_dict[\"model_name\"]\n",
    "pretrained = config_dict[\"pretrained\"]\n",
    "data_train_path = config_dict[\"data_train_path\"]\n",
    "data_validation_path = config_dict[\"data_validation_path\"]\n",
    "\n",
    "\n",
    "output_data_dir = config_dict[\"output_data_dir\"] + \"/\"\n",
    "output_model_dir = config_dict[\"output_model_dir\"] + \"/\"\n",
    "\n",
    "hyperparameters = config_dict[\"training_settings\"][\"hyperparameters\"]\n",
    "hyperparameters[\"learning_rate\"] = float(hyperparameters[\"learning_rate\"])\n",
    "hyperparameters[\"weight_decay\"] = float(hyperparameters[\"weight_decay\"])\n",
    "\n",
    "deepspeed_config = config_dict[\"training_settings\"][\"deepspeed_settings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5e37c038-3708-4c95-97d6-74f362438bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'find_hyperparams_automatically': False,\n",
       " 'num_trials': 10,\n",
       " 'fp16': True,\n",
       " 'deepspeed': True,\n",
       " 'grad_accumulation_steps': 2,\n",
       " 'eval_batch_size': 1,\n",
       " 'learning_rate': 0.0003,\n",
       " 'epochs': 10,\n",
       " 'warmup_steps': 100,\n",
       " 'epsilon': '1e-7',\n",
       " 'batch_size': 1,\n",
       " 'sample_every': 100,\n",
       " 'seed': 42,\n",
       " 'eval_steps': 10,\n",
       " 'weight_decay': 0.01}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "699dc699-3354-4f52-b336-1d87d7cd39a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = hyperparameters['learning_rate']\n",
    "epsilon = float(hyperparameters['epsilon'])  # epsilon must be a float, not str\n",
    "epochs = hyperparameters['epochs']\n",
    "warmup_steps = float(hyperparameters['warmup_steps'])\n",
    "sample_every = float(hyperparameters['sample_every'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f16f95bc-c5d9-42ee-a70a-ced0d5768ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "# For gradient checkpointing, use HF training\n",
    "# I tried custom gradient checkpointing, but am getting issues with HF ecosystem + pytorch checkpointing\n",
    "# HF repository https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n",
    "# shows gradient checkpointing method, later i'll try pytorch lightning\n",
    "\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(output_dir=config_dict['output_model_dir'],\n",
    "                                overwrite_output_dir=True,\n",
    "                                fp16 = config_dict['training_settings']['hyperparameters'][\"fp16\"],\n",
    "                                #deepspeed=config_dict['training_settings']['deepspeed_settings'],\n",
    "                                evaluation_strategy = \"steps\", # used to be epoch\n",
    "                                prediction_loss_only = True, #get rid of this if we end up adding metrics\n",
    "                                logging_dir=f\"./logs/\",\n",
    "                                logging_strategy=\"steps\",\n",
    "                                logging_steps=5,\n",
    "                                save_strategy=\"no\",\n",
    "                                per_device_eval_batch_size=config_dict['training_settings']['hyperparameters'][\"eval_batch_size\"],\n",
    "                                gradient_accumulation_steps=config_dict['training_settings']['hyperparameters'][\"grad_accumulation_steps\"],\n",
    "                                learning_rate=config_dict['training_settings']['hyperparameters'][\"learning_rate\"],\n",
    "                                weight_decay=config_dict['training_settings']['hyperparameters'][\"weight_decay\"],\n",
    "                                num_train_epochs=config_dict['training_settings']['hyperparameters'][\"epochs\"],\n",
    "                                seed=config_dict['training_settings']['hyperparameters'][\"seed\"],\n",
    "                                per_device_train_batch_size=config_dict['training_settings']['hyperparameters'][\"batch_size\"],\n",
    "                                eval_steps=config_dict['training_settings']['hyperparameters'][\"eval_steps\"]\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e1c8be8d-ca3b-4ac8-b2b1-94ec056f29b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "# constructs the Trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b686e7-7e23-4aaa-aa47-06d1316bb493",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/torch_p37/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 808\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 4040\n",
      "  Number of trainable parameters = 354823168\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2855' max='4040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2855/4040 28:31 < 11:51, 1.67 it/s, Epoch 7.06/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.298400</td>\n",
       "      <td>1.911232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.007600</td>\n",
       "      <td>1.723112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.035900</td>\n",
       "      <td>1.634935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.677200</td>\n",
       "      <td>1.605397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.860800</td>\n",
       "      <td>1.475746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.875200</td>\n",
       "      <td>1.570996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.571100</td>\n",
       "      <td>1.442065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.534400</td>\n",
       "      <td>1.453672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.291500</td>\n",
       "      <td>1.449032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.581500</td>\n",
       "      <td>1.488120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.790700</td>\n",
       "      <td>1.464702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.509900</td>\n",
       "      <td>1.514575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.041200</td>\n",
       "      <td>1.456544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.635800</td>\n",
       "      <td>1.454052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.391300</td>\n",
       "      <td>1.377792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.582300</td>\n",
       "      <td>1.356735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.364600</td>\n",
       "      <td>1.312779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.259700</td>\n",
       "      <td>1.287077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.361700</td>\n",
       "      <td>1.313134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.397700</td>\n",
       "      <td>1.277767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.812700</td>\n",
       "      <td>1.282509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.738300</td>\n",
       "      <td>1.282373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.230200</td>\n",
       "      <td>1.220994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.086000</td>\n",
       "      <td>1.216114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.150800</td>\n",
       "      <td>1.234642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.991100</td>\n",
       "      <td>1.259521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.722300</td>\n",
       "      <td>1.226436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.336500</td>\n",
       "      <td>1.194984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.238400</td>\n",
       "      <td>1.204490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.168500</td>\n",
       "      <td>1.217300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.296100</td>\n",
       "      <td>1.198468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.096600</td>\n",
       "      <td>1.229203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.393400</td>\n",
       "      <td>1.233377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.518600</td>\n",
       "      <td>1.218196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.974900</td>\n",
       "      <td>1.176270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.118100</td>\n",
       "      <td>1.196463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.204200</td>\n",
       "      <td>1.185123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.666300</td>\n",
       "      <td>1.162264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>1.365100</td>\n",
       "      <td>1.168058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.013000</td>\n",
       "      <td>1.187763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.739100</td>\n",
       "      <td>1.171356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.866300</td>\n",
       "      <td>1.232688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.894700</td>\n",
       "      <td>1.191733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>1.002600</td>\n",
       "      <td>1.176519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.973100</td>\n",
       "      <td>1.115497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.915300</td>\n",
       "      <td>1.144043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.599900</td>\n",
       "      <td>1.148731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.929200</td>\n",
       "      <td>1.155089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.631300</td>\n",
       "      <td>1.171078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.869300</td>\n",
       "      <td>1.138765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.903500</td>\n",
       "      <td>1.112475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>1.005100</td>\n",
       "      <td>1.127357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.835600</td>\n",
       "      <td>1.096027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.709500</td>\n",
       "      <td>1.094856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.876100</td>\n",
       "      <td>1.118750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.714100</td>\n",
       "      <td>1.105732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.807800</td>\n",
       "      <td>1.097235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.603400</td>\n",
       "      <td>1.088425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.726100</td>\n",
       "      <td>1.094349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.816800</td>\n",
       "      <td>1.109672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.849400</td>\n",
       "      <td>1.149557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.945800</td>\n",
       "      <td>1.108263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>1.001300</td>\n",
       "      <td>1.072716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.725400</td>\n",
       "      <td>1.084131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.876300</td>\n",
       "      <td>1.115114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.607000</td>\n",
       "      <td>1.095983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.965800</td>\n",
       "      <td>1.101062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.597500</td>\n",
       "      <td>1.065206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.969500</td>\n",
       "      <td>1.079861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.964700</td>\n",
       "      <td>1.062439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.679800</td>\n",
       "      <td>1.054210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>1.095400</td>\n",
       "      <td>1.051759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.897400</td>\n",
       "      <td>1.080640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>1.030800</td>\n",
       "      <td>1.051301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.058700</td>\n",
       "      <td>1.026135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.759600</td>\n",
       "      <td>0.995475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.628900</td>\n",
       "      <td>1.009483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>1.018275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.857800</td>\n",
       "      <td>1.002757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.713100</td>\n",
       "      <td>0.983250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.702000</td>\n",
       "      <td>1.019239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.338400</td>\n",
       "      <td>1.065050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.481500</td>\n",
       "      <td>1.050809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.437400</td>\n",
       "      <td>1.021938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.473400</td>\n",
       "      <td>1.042562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.403200</td>\n",
       "      <td>1.060679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>1.084791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.352900</td>\n",
       "      <td>1.084341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.572000</td>\n",
       "      <td>1.035184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.358100</td>\n",
       "      <td>1.031367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.458900</td>\n",
       "      <td>1.055242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.581100</td>\n",
       "      <td>1.026736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.493900</td>\n",
       "      <td>1.038933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.558100</td>\n",
       "      <td>1.087781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.506500</td>\n",
       "      <td>1.023838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.479500</td>\n",
       "      <td>1.028120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>0.468200</td>\n",
       "      <td>1.058670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.525800</td>\n",
       "      <td>1.057199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.538400</td>\n",
       "      <td>1.024382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.579000</td>\n",
       "      <td>1.000305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>0.647100</td>\n",
       "      <td>1.017330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.536900</td>\n",
       "      <td>1.033937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>0.528200</td>\n",
       "      <td>1.035495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.489000</td>\n",
       "      <td>1.014019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.560300</td>\n",
       "      <td>1.036940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.428400</td>\n",
       "      <td>1.037402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>0.464600</td>\n",
       "      <td>0.999830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.415000</td>\n",
       "      <td>1.006620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>0.422600</td>\n",
       "      <td>1.002834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.408300</td>\n",
       "      <td>1.000784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>0.458600</td>\n",
       "      <td>1.015554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.985493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>0.564800</td>\n",
       "      <td>0.954104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.567700</td>\n",
       "      <td>0.961253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.461900</td>\n",
       "      <td>0.969388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.597000</td>\n",
       "      <td>1.001004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>0.511900</td>\n",
       "      <td>0.980224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>0.991362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>0.516000</td>\n",
       "      <td>1.033756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.408600</td>\n",
       "      <td>1.003685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>0.430900</td>\n",
       "      <td>0.969356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>0.277200</td>\n",
       "      <td>0.999628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>0.309300</td>\n",
       "      <td>1.049268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>0.303800</td>\n",
       "      <td>1.068298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.397500</td>\n",
       "      <td>1.030051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>0.345900</td>\n",
       "      <td>1.021295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>0.284700</td>\n",
       "      <td>1.025092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.333400</td>\n",
       "      <td>1.027819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>0.288800</td>\n",
       "      <td>1.055763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.407800</td>\n",
       "      <td>1.062574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>0.278900</td>\n",
       "      <td>1.017490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.371200</td>\n",
       "      <td>1.031132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>1.045745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>0.238900</td>\n",
       "      <td>1.028018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.365400</td>\n",
       "      <td>0.994480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.336400</td>\n",
       "      <td>0.994896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>0.354800</td>\n",
       "      <td>1.028844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>0.292300</td>\n",
       "      <td>1.057810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>0.322600</td>\n",
       "      <td>1.072262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.389000</td>\n",
       "      <td>1.010652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>0.335100</td>\n",
       "      <td>0.969317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>0.351500</td>\n",
       "      <td>0.983299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>0.315800</td>\n",
       "      <td>0.992861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.323700</td>\n",
       "      <td>0.994266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.366400</td>\n",
       "      <td>0.990051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>0.350700</td>\n",
       "      <td>1.030766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>0.363000</td>\n",
       "      <td>1.037183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>0.481300</td>\n",
       "      <td>0.994642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>0.365100</td>\n",
       "      <td>0.964196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.342900</td>\n",
       "      <td>0.980153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1510</td>\n",
       "      <td>0.398100</td>\n",
       "      <td>0.994299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.414600</td>\n",
       "      <td>0.990502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1530</td>\n",
       "      <td>0.295000</td>\n",
       "      <td>0.994406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>0.432500</td>\n",
       "      <td>1.018127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.391200</td>\n",
       "      <td>0.949085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>0.393900</td>\n",
       "      <td>0.931679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1570</td>\n",
       "      <td>0.319700</td>\n",
       "      <td>0.957760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>0.319100</td>\n",
       "      <td>0.957232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1590</td>\n",
       "      <td>0.307400</td>\n",
       "      <td>0.984808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.280600</td>\n",
       "      <td>1.030898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1610</td>\n",
       "      <td>0.279600</td>\n",
       "      <td>1.046453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>0.254700</td>\n",
       "      <td>1.043363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1630</td>\n",
       "      <td>0.252400</td>\n",
       "      <td>1.025246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>0.253900</td>\n",
       "      <td>1.029731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.227700</td>\n",
       "      <td>1.056433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>1.055742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1670</td>\n",
       "      <td>0.244800</td>\n",
       "      <td>1.020429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.278300</td>\n",
       "      <td>1.010479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1690</td>\n",
       "      <td>0.228800</td>\n",
       "      <td>1.067109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.272400</td>\n",
       "      <td>1.125002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1710</td>\n",
       "      <td>0.293500</td>\n",
       "      <td>1.121615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>0.295400</td>\n",
       "      <td>1.042070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1730</td>\n",
       "      <td>0.330500</td>\n",
       "      <td>1.028583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>0.289200</td>\n",
       "      <td>1.043249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.254400</td>\n",
       "      <td>1.028255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.281900</td>\n",
       "      <td>1.015391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1770</td>\n",
       "      <td>0.241000</td>\n",
       "      <td>1.042722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>0.289000</td>\n",
       "      <td>1.053891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1790</td>\n",
       "      <td>0.260800</td>\n",
       "      <td>1.028214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.258900</td>\n",
       "      <td>1.009603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1810</td>\n",
       "      <td>0.231800</td>\n",
       "      <td>1.012224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>0.241700</td>\n",
       "      <td>1.010215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1830</td>\n",
       "      <td>0.316600</td>\n",
       "      <td>1.024438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.278700</td>\n",
       "      <td>1.034228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.230800</td>\n",
       "      <td>1.049428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>0.254700</td>\n",
       "      <td>1.054775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1870</td>\n",
       "      <td>0.280700</td>\n",
       "      <td>1.079905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>0.336200</td>\n",
       "      <td>1.057392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1890</td>\n",
       "      <td>0.283900</td>\n",
       "      <td>1.061694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.280400</td>\n",
       "      <td>1.059336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1910</td>\n",
       "      <td>0.257400</td>\n",
       "      <td>1.025550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.310900</td>\n",
       "      <td>1.033797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1930</td>\n",
       "      <td>0.172300</td>\n",
       "      <td>1.064102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1940</td>\n",
       "      <td>0.276100</td>\n",
       "      <td>1.104366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.213500</td>\n",
       "      <td>1.085109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>0.334900</td>\n",
       "      <td>1.098073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1970</td>\n",
       "      <td>0.175200</td>\n",
       "      <td>1.069834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1980</td>\n",
       "      <td>0.234200</td>\n",
       "      <td>1.046722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1990</td>\n",
       "      <td>0.325600</td>\n",
       "      <td>1.047070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.226800</td>\n",
       "      <td>1.029896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2010</td>\n",
       "      <td>0.261600</td>\n",
       "      <td>1.064632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020</td>\n",
       "      <td>0.239800</td>\n",
       "      <td>1.057562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2030</td>\n",
       "      <td>0.204000</td>\n",
       "      <td>1.056242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>0.216500</td>\n",
       "      <td>1.079846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.229300</td>\n",
       "      <td>1.099744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2060</td>\n",
       "      <td>0.222200</td>\n",
       "      <td>1.102796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2070</td>\n",
       "      <td>0.180200</td>\n",
       "      <td>1.107818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.245400</td>\n",
       "      <td>1.122324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2090</td>\n",
       "      <td>0.192300</td>\n",
       "      <td>1.105007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.205400</td>\n",
       "      <td>1.085118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2110</td>\n",
       "      <td>0.275100</td>\n",
       "      <td>1.067889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2120</td>\n",
       "      <td>0.192900</td>\n",
       "      <td>1.056157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2130</td>\n",
       "      <td>0.211100</td>\n",
       "      <td>1.062824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2140</td>\n",
       "      <td>0.235600</td>\n",
       "      <td>1.073002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.206000</td>\n",
       "      <td>1.057105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>1.053020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2170</td>\n",
       "      <td>0.213900</td>\n",
       "      <td>1.101893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2180</td>\n",
       "      <td>0.249400</td>\n",
       "      <td>1.099178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2190</td>\n",
       "      <td>0.199900</td>\n",
       "      <td>1.101831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.232200</td>\n",
       "      <td>1.116038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2210</td>\n",
       "      <td>0.229700</td>\n",
       "      <td>1.126880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2220</td>\n",
       "      <td>0.221100</td>\n",
       "      <td>1.124387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2230</td>\n",
       "      <td>0.206200</td>\n",
       "      <td>1.114921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.208700</td>\n",
       "      <td>1.100723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.238000</td>\n",
       "      <td>1.086767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2260</td>\n",
       "      <td>0.252000</td>\n",
       "      <td>1.088213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2270</td>\n",
       "      <td>0.180800</td>\n",
       "      <td>1.121349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>0.261700</td>\n",
       "      <td>1.132603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2290</td>\n",
       "      <td>0.231600</td>\n",
       "      <td>1.101845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.263000</td>\n",
       "      <td>1.083212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2310</td>\n",
       "      <td>0.227300</td>\n",
       "      <td>1.094889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2320</td>\n",
       "      <td>0.188200</td>\n",
       "      <td>1.100694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2330</td>\n",
       "      <td>0.247700</td>\n",
       "      <td>1.117162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2340</td>\n",
       "      <td>0.218700</td>\n",
       "      <td>1.116785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.226100</td>\n",
       "      <td>1.116130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2360</td>\n",
       "      <td>0.189300</td>\n",
       "      <td>1.084485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2370</td>\n",
       "      <td>0.209400</td>\n",
       "      <td>1.083380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2380</td>\n",
       "      <td>0.185700</td>\n",
       "      <td>1.089131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2390</td>\n",
       "      <td>0.236300</td>\n",
       "      <td>1.070127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.233400</td>\n",
       "      <td>1.049628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2410</td>\n",
       "      <td>0.192400</td>\n",
       "      <td>1.058576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2420</td>\n",
       "      <td>0.216500</td>\n",
       "      <td>1.081244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2430</td>\n",
       "      <td>0.156600</td>\n",
       "      <td>1.083072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2440</td>\n",
       "      <td>0.183900</td>\n",
       "      <td>1.095092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.232600</td>\n",
       "      <td>1.093289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2460</td>\n",
       "      <td>0.234500</td>\n",
       "      <td>1.079746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2470</td>\n",
       "      <td>0.183500</td>\n",
       "      <td>1.088691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2480</td>\n",
       "      <td>0.177600</td>\n",
       "      <td>1.096046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2490</td>\n",
       "      <td>0.183800</td>\n",
       "      <td>1.104083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.186100</td>\n",
       "      <td>1.110756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2510</td>\n",
       "      <td>0.191300</td>\n",
       "      <td>1.122168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>0.234200</td>\n",
       "      <td>1.121391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2530</td>\n",
       "      <td>0.201600</td>\n",
       "      <td>1.082210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2540</td>\n",
       "      <td>0.185000</td>\n",
       "      <td>1.036267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.178000</td>\n",
       "      <td>1.036884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2560</td>\n",
       "      <td>0.180800</td>\n",
       "      <td>1.081473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2570</td>\n",
       "      <td>0.203500</td>\n",
       "      <td>1.123208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2580</td>\n",
       "      <td>0.223600</td>\n",
       "      <td>1.140105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2590</td>\n",
       "      <td>0.191300</td>\n",
       "      <td>1.116377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.185300</td>\n",
       "      <td>1.118860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2610</td>\n",
       "      <td>0.165900</td>\n",
       "      <td>1.146856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2620</td>\n",
       "      <td>0.180200</td>\n",
       "      <td>1.136920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2630</td>\n",
       "      <td>0.164600</td>\n",
       "      <td>1.139375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2640</td>\n",
       "      <td>0.177300</td>\n",
       "      <td>1.158866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.246700</td>\n",
       "      <td>1.141543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2660</td>\n",
       "      <td>0.169200</td>\n",
       "      <td>1.099015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2670</td>\n",
       "      <td>0.193700</td>\n",
       "      <td>1.100562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2680</td>\n",
       "      <td>0.189100</td>\n",
       "      <td>1.111929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2690</td>\n",
       "      <td>0.181900</td>\n",
       "      <td>1.123644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.219700</td>\n",
       "      <td>1.141858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2710</td>\n",
       "      <td>0.208700</td>\n",
       "      <td>1.131739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2720</td>\n",
       "      <td>0.191100</td>\n",
       "      <td>1.146533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2730</td>\n",
       "      <td>0.165000</td>\n",
       "      <td>1.137478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2740</td>\n",
       "      <td>0.168700</td>\n",
       "      <td>1.135391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.187900</td>\n",
       "      <td>1.143658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2760</td>\n",
       "      <td>0.225300</td>\n",
       "      <td>1.147239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2770</td>\n",
       "      <td>0.171100</td>\n",
       "      <td>1.136705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2780</td>\n",
       "      <td>0.137200</td>\n",
       "      <td>1.140316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2790</td>\n",
       "      <td>0.191100</td>\n",
       "      <td>1.134618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.197100</td>\n",
       "      <td>1.144412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2810</td>\n",
       "      <td>0.232100</td>\n",
       "      <td>1.133589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2820</td>\n",
       "      <td>0.217300</td>\n",
       "      <td>1.129033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2830</td>\n",
       "      <td>0.173600</td>\n",
       "      <td>1.128558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2840</td>\n",
       "      <td>0.147500</td>\n",
       "      <td>1.126807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.159800</td>\n",
       "      <td>1.138764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 95\n",
      "  Batch size = 1\n"
     ]
    }
   ],
   "source": [
    "trainer.train() #training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b402779-1c79-4c65-8f92-90686ce1ca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Results \n",
    "trainer.save_model(self.output_model_dir) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
