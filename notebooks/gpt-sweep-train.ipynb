{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c6622e2-24df-40eb-ad1b-01823c25f803",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/torch_p37/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# wandb for tracking experiment and sweeping hyperparamters\n",
    "import wandb\n",
    "# pyyaml (yaml) :: parses configuration files (YAML files)\n",
    "# see https://www.cloudbees.com/blog/yaml-tutorial-everything-you-need-get-started for more information on YAML files\n",
    "import yaml \n",
    "\n",
    "# huggingface :: datasets : dataset-handling libraries from huggingface\n",
    "from datasets import load_dataset\n",
    "#from datasets.filesystems import S3FileSystem # for S3 interactions\n",
    "\n",
    "# huggingface :: transformers : transformer, trainer and tokenizer objects for the actual training\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config, AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "027eaab0-abc3-4cec-b375-d5bdf2245067",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk punkt sentence tokenizer, divides text into a list of sentences\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bef5de79-8cc4-4b63-b5d5-a2bb874ea8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb 22 02:00:36 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   34C    P8    14W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39fb8da1-29b8-423b-b888-60fe9b486e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_config = \"../configs/gpt2-refl-21-feb-2023.yaml\"\n",
    "# open yaml config as a strema and load into config_dict\n",
    "with open(path_to_config, \"r\") as stream:\n",
    "    try:\n",
    "        config_dict = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(\"Configuration load failed!\")\n",
    "        print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "607b596e-762a-4735-a413-00f094c030b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(config_dict[\"data_train_path\"])\n",
    "df_val = pd.read_csv(config_dict[\"data_validation_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d46f7a90-2ddc-4d32-9b54-de380dcfb35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)  # drop NA values\n",
    "triplets = df.triplet.copy()  # copy over triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc0eea85-8da6-4337-8aca-e8945c78c2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_triplets = df_val.triplet.copy()  # validation triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "839c2a2e-7c5c-4031-9a15-a929c4e3cd94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7f7240961950>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAekAAAHpCAYAAACmzsSXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAoX0lEQVR4nO3df3RU9Z3/8ddAfmpIAgkkQTMkIJKAgghuGLS7FVKRVYtLTqsU2ii03boRgeyukipGaBW2PQVkG2FxEbZbKZU9QsEWqAaNdRsiRFFSQ4SKDoUkNGAyAcIkJJ/vHy7zZQpYEicznwnPxzn3HHPv8OF9Oz0+nZl7Mw5jjBEAALBOr1APAAAALo5IAwBgKSINAICliDQAAJYi0gAAWIpIAwBgKSINAIClenykjTHyeDzidnAAQLjp8ZFubm5WQkKCmpubQz0KAACd0uMjDQBAuCLSAABYikgDAGApIg0AgKWINAAAliLSAABYikgDAGApIg0AgKWINAAAliLSAABYikgDAGApIg0AgKWINAAAliLSAABYikgDAGApIg0AgKWINAAAliLSAABYikgDAGCpiFAPgPDidrvV0NAQ8HWTk5PldDoDvi4AhDMijcvmdruVlZWtlpbTAV87NvYq7d9fTagB4DxEGpetoaFBLS2nlTOzWPFpGQFb11P7sSpeWKiGhgYiDQDnIdLotPi0DPVzDgv1GADQ43HhGAAAlgp5pI8cOaIZM2YoKSlJsbGxuvHGG7Vnzx7fcWOMnnzySaWlpSk2Nla5ubk6cOBACCcGACA4QhrpTz/9VLfeeqsiIyO1bds2ffDBB/rJT36ivn37+h7zox/9SCtWrNCqVatUUVGhq6++WpMmTdKZM2dCODkAAN0vpJ9J/9u//ZvS09O1du1a377MzEzfPxtjtHz5cj3xxBOaMmWKJOlnP/uZUlJStHnzZt1///0XrOn1euX1en0/ezyebjwDAAC6T0hfSW/ZskVjx47V1772NQ0YMECjR4/W888/7zt+6NAh1dXVKTc317cvISFBOTk5Ki8vv+iaixcvVkJCgm9LT0/v9vMAAKA7hDTSH330kVauXKmhQ4dqx44deuihh/TII4/ov/7rvyRJdXV1kqSUlBS/P5eSkuI79peKiorU1NTk2w4fPty9JwEAQDcJ6dvdHR0dGjt2rJ555hlJ0ujRo1VVVaVVq1YpPz+/S2tGR0crOjo6kGMCABASIX0lnZaWpuHDh/vty87OltvtliSlpqZKkurr6/0eU19f7zsGAEBPFdJI33rrraqpqfHb9+GHH2rQoEGSPruILDU1VaWlpb7jHo9HFRUVcrlcQZ0VAIBgC+nb3fPmzdP48eP1zDPP6Otf/7refvttrV69WqtXr5YkORwOzZ07Vz/84Q81dOhQZWZmasGCBRo4cKDuvffeUI4OAEC3C2mkb7nlFm3atElFRUVatGiRMjMztXz5ck2fPt33mEcffVSnTp3Sd7/7XTU2Nuq2227T9u3bFRMTE8LJAQDofiH/3d1333237r777ksedzgcWrRokRYtWhTEqQAACL2Q/1pQAABwcUQaAABLEWkAACxFpAEAsBSRBgDAUkQaAABLEWkAACxFpAEAsBSRBgDAUkQaAABLEWkAACxFpAEAsBSRBgDAUkQaAABLEWkAACxFpAEAsBSRBgDAUkQaAABLEWkAACxFpAEAsBSRBgDAUkQaAABLEWkAACxFpAEAsBSRBgDAUkQaAABLEWkAACxFpAEAsBSRBgDAUkQaAABLEWkAACxFpAEAsBSRBgDAUkQaAABLEWkAACxFpAEAsBSRBgDAUkQaAABLEWkAACxFpAEAsBSRBgDAUkQaAABLEWkAACxFpAEAsBSRBgDAUkQaAABLEWkAACxFpAEAsBSRBgDAUkQaAABLEWkAACxFpAEAsBSRBgDAUkQaAABLEWkAACxFpAEAsBSRBgDAUkQaAABLhTTSTz31lBwOh9+WlZXlO37mzBkVFBQoKSlJcXFxysvLU319fQgnBgAgeEL+SnrEiBGqra31bW+99Zbv2Lx587R161Zt3LhRZWVlOnr0qKZOnRrCaQEACJ6IkA8QEaHU1NQL9jc1NWnNmjVav369JkyYIElau3atsrOztWvXLo0bNy7YowIAEFQhfyV94MABDRw4UIMHD9b06dPldrslSZWVlWpra1Nubq7vsVlZWXI6nSovL7/kel6vVx6Px28DACAchTTSOTk5WrdunbZv366VK1fq0KFD+tKXvqTm5mbV1dUpKipKiYmJfn8mJSVFdXV1l1xz8eLFSkhI8G3p6endfBYAAHSPkL7dPXnyZN8/jxw5Ujk5ORo0aJBeeuklxcbGdmnNoqIiFRYW+n72eDyEGgAQlkL+dvf5EhMTdf311+vgwYNKTU1Va2urGhsb/R5TX19/0c+wz4mOjlZ8fLzfBgBAOLIq0idPntQf//hHpaWlacyYMYqMjFRpaanveE1Njdxut1wuVwinBAAgOEL6dve//Mu/6J577tGgQYN09OhRFRcXq3fv3po2bZoSEhI0a9YsFRYWql+/foqPj9fs2bPlcrm4shsAcEUIaaT/9Kc/adq0aTp+/Lj69++v2267Tbt27VL//v0lScuWLVOvXr2Ul5cnr9erSZMm6bnnngvlyAAABE1II71hw4bPPR4TE6OSkhKVlJQEaSIAAOxh1WfSAADg/yPSAABYikgDAGApIg0AgKWINAAAliLSAABYikgDAGApIg0AgKWINAAAliLSAABYikgDAGApIg0AgKWINAAAliLSAABYikgDAGApIg0AgKWINAAAliLSAABYikgDAGApIg0AgKWINAAAliLSAABYikgDAGApIg0AgKWINAAAliLSAABYikgDAGApIg0AgKWINAAAliLSAABYikgDAGApIg0AgKWINAAAliLSAABYikgDAGApIg0AgKWINAAAliLSAABYikgDAGApIg0AgKWINAAAliLSAABYikgDAGApIg0AgKWINAAAliLSAABYikgDAGApIg0AgKWINAAAliLSAABYikgDAGApIg0AgKWINAAAliLSAABYikgDAGApIg0AgKWINAAAliLSAABYikgDAGApayK9ZMkSORwOzZ0717fvzJkzKigoUFJSkuLi4pSXl6f6+vrQDQkAQBBZEendu3frP/7jPzRy5Ei//fPmzdPWrVu1ceNGlZWV6ejRo5o6dWqIpgQAILhCHumTJ09q+vTpev7559W3b1/f/qamJq1Zs0ZLly7VhAkTNGbMGK1du1a///3vtWvXrhBODABAcIQ80gUFBbrrrruUm5vrt7+yslJtbW1++7OysuR0OlVeXn7J9bxerzwej98GAEA4igjlX75hwwa988472r179wXH6urqFBUVpcTERL/9KSkpqquru+Saixcv1sKFCwM9KgAAQReyV9KHDx/WnDlz9OKLLyomJiZg6xYVFampqcm3HT58OGBrAwAQTCGLdGVlpY4dO6abb75ZERERioiIUFlZmVasWKGIiAilpKSotbVVjY2Nfn+uvr5eqampl1w3Ojpa8fHxfhsAAOEoZG93T5w4Ufv27fPb9+CDDyorK0uPPfaY0tPTFRkZqdLSUuXl5UmSampq5Ha75XK5QjEyAABBFbJI9+nTRzfccIPfvquvvlpJSUm+/bNmzVJhYaH69eun+Ph4zZ49Wy6XS+PGjQvFyAAABFVILxz7a5YtW6ZevXopLy9PXq9XkyZN0nPPPRfqsQAACAqrIv3GG2/4/RwTE6OSkhKVlJSEZiAAAEIo5PdJAwCAiyPSAABYikgDAGApIg0AgKWINAAAliLSAABYyqpbsBBYbrdbDQ0NAVuvuro6YGsFY/3k5GQ5nc6ArgkAwUSkeyi3262srGy1tJwO+Npt3taArtfSdFySQzNmzAjourGxV2n//mpCDSBsEekeqqGhQS0tp5Uzs1jxaRkBWbN2X7mqtqzW2bNnA7LeOW2nmyUZ3fSNx9Q/Mysga3pqP1bFCwvV0NBApAGELSLdw8WnZaifc1hA1vLUfhyQdS4lboAzYLMCQE/AhWMAAFiKSAMAYCkiDQCApYg0AACWItIAAFiKSAMAYCkiDQCApYg0AACWItIAAFiKSAMAYCkiDQCApYg0AACWItIAAFiKSAMAYCkiDQCApYg0AACWigj1AEB3qq6uDuh6ycnJcjqdAV0TAC6FSKNHamk6LsmhGTNmBHTd2NirtH9/NaEGEBREGj1S2+lmSUY3feMx9c/MCsiantqPVfHCQjU0NBBpAEFBpNGjxQ1wqp9zWKjHAIAu4cIxAAAs1aVIDx48WMePH79gf2NjowYPHvyFhwIAAF2M9Mcff6z29vYL9nu9Xh05cuQLDwUAADr5mfSWLVt8/7xjxw4lJCT4fm5vb1dpaakyMjICNhwAAFeyTkX63nvvlSQ5HA7l5+f7HYuMjFRGRoZ+8pOfBGw4AACuZJ2KdEdHhyQpMzNTu3fvVnJycrcMBQAAungL1qFDhwI9BwAA+Atdvk+6tLRUpaWlOnbsmO8V9jkvvPDCFx4MAIArXZcivXDhQi1atEhjx45VWlqaHA5HoOcCAOCK16VIr1q1SuvWrdM3v/nNQM8DAAD+T5fuk25tbdX48eMDPQsAADhPlyL97W9/W+vXrw/0LAAA4Dxderv7zJkzWr16tV577TWNHDlSkZGRfseXLl0akOEAALiSdSnS77//vm666SZJUlVVld8xLiIDACAwuhTp119/PdBzAACAv8BXVQIAYKkuvZK+/fbbP/dt7Z07d3Z5IAAA8JkuRfrc59HntLW1ae/evaqqqrrgizcAAEDXdCnSy5Ytu+j+p556SidPnvxCAwEAgM8E9DPpGTNm8Hu7AQAIkIBGury8XDExMYFcEgCAK1aX3u6eOnWq38/GGNXW1mrPnj1asGBBQAYDAOBK16VIJyQk+P3cq1cvDRs2TIsWLdIdd9wRkMEAALjSdSnSa9euDfQcAADgL3Qp0udUVlaqurpakjRixAiNHj06IEMBAIAuRvrYsWO6//779cYbbygxMVGS1NjYqNtvv10bNmxQ//79AzkjAABXpC5d3T179mw1NzfrD3/4g06cOKETJ06oqqpKHo9HjzzySKBnBADgitSlV9Lbt2/Xa6+9puzsbN++4cOHq6SkhAvHAAAIkC69ku7o6LjgO6QlKTIyUh0dHV94KAAA0MVIT5gwQXPmzNHRo0d9+44cOaJ58+Zp4sSJl73OypUrNXLkSMXHxys+Pl4ul0vbtm3zHT9z5owKCgqUlJSkuLg45eXlqb6+visjAwAQdroU6Z/+9KfyeDzKyMjQkCFDNGTIEGVmZsrj8ejf//3fL3uda6+9VkuWLFFlZaX27NmjCRMmaMqUKfrDH/4gSZo3b562bt2qjRs3qqysTEePHr3gF6kAANBTdekz6fT0dL3zzjt67bXXtH//fklSdna2cnNzO7XOPffc4/fz008/rZUrV2rXrl269tprtWbNGq1fv14TJkyQ9Nn92dnZ2dq1a5fGjRt30TW9Xq+8Xq/vZ4/H06mZAACwRadeSe/cuVPDhw+Xx+ORw+HQV77yFc2ePVuzZ8/WLbfcohEjRuh3v/tdlwZpb2/Xhg0bdOrUKblcLlVWVqqtrc0v/FlZWXI6nSovL7/kOosXL1ZCQoJvS09P79I8AACEWqcivXz5cn3nO99RfHz8BccSEhL0j//4j1q6dGmnBti3b5/i4uIUHR2t733ve9q0aZOGDx+uuro6RUVF+e7DPiclJUV1dXWXXK+oqEhNTU2+7fDhw52aBwAAW3Qq0u+9957uvPPOSx6/4447VFlZ2akBhg0bpr1796qiokIPPfSQ8vPz9cEHH3RqjfNFR0f7LkQ7twEAEI469Zl0fX39RW+98i0WEaE///nPnRogKipK1113nSRpzJgx2r17t5599lndd999am1tVWNjo9+r6fr6eqWmpnbq7wAAIBx16pX0Nddco6qqqksef//995WWlvaFBuro6JDX69WYMWMUGRmp0tJS37Gamhq53W65XK4v9HcAABAOOvVK+u///u+1YMEC3XnnnYqJifE71tLSouLiYt19992XvV5RUZEmT54sp9Op5uZmrV+/Xm+88YZ27NihhIQEzZo1S4WFherXr5/i4+M1e/ZsuVyuS17ZDQBAT9KpSD/xxBN6+eWXdf311+vhhx/WsGHDJEn79+9XSUmJ2tvb9fjjj1/2eseOHdO3vvUt1dbWKiEhQSNHjtSOHTv0la98RZK0bNky9erVS3l5efJ6vZo0aZKee+65zowMAEDY6lSkU1JS9Pvf/14PPfSQioqKZIyRJDkcDk2aNEklJSVKSUm57PXWrFnzucdjYmJUUlKikpKSzowJAECP0OlfZjJo0CD95je/0aeffqqDBw/KGKOhQ4eqb9++3TEfAABXrC79xjFJ6tu3r2655ZZAzgIAAM7Tpd/dDQAAuh+RBgDAUkQaAABLEWkAACxFpAEAsBSRBgDAUkQaAABLEWkAACxFpAEAsBSRBgDAUl3+taAILLfbrYaGhoCtV11dHbC10P0C/fxLUnJyspxOZ0DXBBBcRNoCbrdbWVnZamk5HfC127ytAV8TgdVdz39s7FXav7+aUANhjEhboKGhQS0tp5Uzs1jxaRkBWbN2X7mqtqzW2bNnA7Ieuk93PP+e2o9V8cJCNTQ0EGkgjBFpi8SnZaifc1hA1vLUfhyQdRA8gXz+AfQMXDgGAICliDQAAJYi0gAAWIpIAwBgKSINAICliDQAAJYi0gAAWIpIAwBgKSINAICliDQAAJYi0gAAWIpIAwBgKSINAICliDQAAJYi0gAAWIpIAwBgKSINAICliDQAAJYi0gAAWIpIAwBgKSINAICliDQAAJYi0gAAWIpIAwBgqYhQDwCEm+rqaqvXA9BzEGngMrU0HZfk0IwZM7pl/TZva7esCyB8EWngMrWdbpZkdNM3HlP/zKyArVu7r1xVW1br7NmzAVsTQM9ApIFOihvgVD/nsICt56n9OGBrAehZuHAMAABLEWkAACxFpAEAsBSRBgDAUkQaAABLEWkAACxFpAEAsBSRBgDAUkQaAABLEWkAACxFpAEAsBSRBgDAUkQaAABLEWkAACwV0kgvXrxYt9xyi/r06aMBAwbo3nvvVU1Njd9jzpw5o4KCAiUlJSkuLk55eXmqr68P0cQAAARPSCNdVlamgoIC7dq1S6+++qra2tp0xx136NSpU77HzJs3T1u3btXGjRtVVlamo0ePaurUqSGcGgCA4IgI5V++fft2v5/XrVunAQMGqLKyUn/7t3+rpqYmrVmzRuvXr9eECRMkSWvXrlV2drZ27dqlcePGhWJsAACCwqrPpJuamiRJ/fr1kyRVVlaqra1Nubm5vsdkZWXJ6XSqvLz8omt4vV55PB6/DQCAcGRNpDs6OjR37lzdeuutuuGGGyRJdXV1ioqKUmJiot9jU1JSVFdXd9F1Fi9erISEBN+Wnp7e3aMDANAtrIl0QUGBqqqqtGHDhi+0TlFRkZqamnzb4cOHAzQhAADBFdLPpM95+OGH9corr+jNN9/Utdde69ufmpqq1tZWNTY2+r2arq+vV2pq6kXXio6OVnR0dHePDABAtwvpK2ljjB5++GFt2rRJO3fuVGZmpt/xMWPGKDIyUqWlpb59NTU1crvdcrlcwR4XAICgCukr6YKCAq1fv16/+tWv1KdPH9/nzAkJCYqNjVVCQoJmzZqlwsJC9evXT/Hx8Zo9e7ZcLhdXdgMAeryQRnrlypWSpC9/+ct++9euXasHHnhAkrRs2TL16tVLeXl58nq9mjRpkp577rkgTwoAQPCFNNLGmL/6mJiYGJWUlKikpCQIEwEAYA9rru4GAAD+iDQAAJYi0gAAWIpIAwBgKSINAICliDQAAJay4teChhO3262GhoaArlldXR3Q9QAAPQOR7gS3262srGy1tJzulvXbvK3dsi4AIDwR6U5oaGhQS8tp5cwsVnxaRsDWrd1Xrqotq3X27NmArQkACH9Eugvi0zLUzzksYOt5aj8O2FoAgJ6DC8cAALAUkQYAwFJEGgAASxFpAAAsRaQBALAUkQYAwFJEGgAASxFpAAAsRaQBALAUkQYAwFJEGgAASxFpAAAsRaQBALAUkQYAwFJEGgAASxFpAAAsRaQBALAUkQYAwFJEGgAASxFpAAAsRaQBALAUkQYAwFJEGgAASxFpAAAsRaQBALAUkQYAwFJEGgAASxFpAAAsRaQBALAUkQYAwFJEGgAASxFpAAAsRaQBALAUkQYAwFJEGgAASxFpAAAsRaQBALAUkQYAwFJEGgAASxFpAAAsRaQBALAUkQYAwFJEGgAASxFpAAAsRaQBALAUkQYAwFJEGgAASxFpAAAsRaQBALBUSCP95ptv6p577tHAgQPlcDi0efNmv+PGGD355JNKS0tTbGyscnNzdeDAgdAMCwBAkIU00qdOndKoUaNUUlJy0eM/+tGPtGLFCq1atUoVFRW6+uqrNWnSJJ05cybIkwIAEHwRofzLJ0+erMmTJ1/0mDFGy5cv1xNPPKEpU6ZIkn72s58pJSVFmzdv1v333x/MUQEACDprP5M+dOiQ6urqlJub69uXkJCgnJwclZeXX/LPeb1eeTwevw0AgHBkbaTr6uokSSkpKX77U1JSfMcuZvHixUpISPBt6enp3TonAADdxdpId1VRUZGampp82+HDh0M9EgAAXWJtpFNTUyVJ9fX1fvvr6+t9xy4mOjpa8fHxfhsAAOHI2khnZmYqNTVVpaWlvn0ej0cVFRVyuVwhnAwAgOAI6dXdJ0+e1MGDB30/Hzp0SHv37lW/fv3kdDo1d+5c/fCHP9TQoUOVmZmpBQsWaODAgbr33ntDNzQAAEES0kjv2bNHt99+u+/nwsJCSVJ+fr7WrVunRx99VKdOndJ3v/tdNTY26rbbbtP27dsVExMTqpEBAAiakEb6y1/+sowxlzzucDi0aNEiLVq0KIhTAQBgB2s/kwYA4EpHpAEAsBSRBgDAUkQaAABLEWkAACxFpAEAsFRIb8EC0L2qq6sDul5ycrKcTmdA1wRwaUQa6IFamo5LcmjGjBkBXTc29irt319NqIEgIdJAD9R2ulmS0U3feEz9M7MCsqan9mNVvLBQDQ0NRBoIEiIN9GBxA5zq5xwW6jEAdBEXjgEAYCkiDQCApYg0AACW4jNpAJ0S6Nu6JG7tAi6FSAO4LN11W5fErV3ApRBpAJelO27rkri1C/g8RBpAp3BbFxA8XDgGAICliDQAAJbi7W4APZLb7VZDQ0PA1+VKdAQTkQbQ47jdbmVlZaul5XTA1+ZKdAQTkQbQ4zQ0NKil5bRyZhYrPi0jYOtyJTqCjUgD6LHi0zK4Eh1hjQvHAACwFJEGAMBSRBoAAEvxmTQAhFh33C7GrWI9A5EGgBDqrtvFuFWsZyDSABBC3XG7GLeK9RxEGgAswO1iuBguHAMAwFJEGgAASxFpAAAsRaQBALAUkQYAwFJEGgAASxFpAAAsRaQBALAUkQYAwFJEGgAAS/FrQQFYobq62sq1gFAi0gBCqqXpuCSHZsyYEfC127ytAV8TCCYiDSCk2k43SzK66RuPqX9mVkDWrN1Xrqotq3X27NmArAeECpEGYIW4Ac6AfQuUp/bjgKwDhBoXjgEAYCkiDQCApYg0AACW4jNpAOihuuNWtOTkZDmdzoCvi4sj0gDQw3TnbW2xsVdp//5qQh0kRBoAepjuuK1N+uyq+YoXFqqhoYFIBwmRBoAeKpC3tSE0uHAMAABLEWkAACxFpAEAsBSfSQNAJ13p39gV6Jm747Yut9uthoaGgK4pBf8WNCINAJfpSv/Gru46/0Df1uV2u5WVla2WltMBWe98wb4FjUgDwGW60r+xqzvOvztu62poaFBLy2nlzCxWfFpGQNaUQnMLGpEGgE660r+xK1xu7YpPywiLOT8PF44BAGCpsIh0SUmJMjIyFBMTo5ycHL399tuhHgkAgG5nfaR/+ctfqrCwUMXFxXrnnXc0atQoTZo0SceOHQv1aAAAdCvrP5NeunSpvvOd7+jBBx+UJK1atUq//vWv9cILL2j+/PkXPN7r9crr9fp+bmpqkiR5PJ4vPMvJkyclSSc+qdFZb8sXXu8cT+0nkqSmIwcUGeG4otbsrnXDZc3uWjdc1uyuda/0WcPq/OvckqTKykrfv2O/qJqaGknd8O/q/5v15MmTAWmKJPXp00cOx+f8b2ks5vV6Te/evc2mTZv89n/rW98yX/3qVy/6Z4qLi40kNjY2NjY267empqbP7aDVr6QbGhrU3t6ulJQUv/0pKSnav3//Rf9MUVGRCgsLfT93dHToxIkTSkpK+vz/WulmHo9H6enpOnz4sOLj40M2xxfFediF87AL52GXcDiPPn36fO5xqyPdFdHR0YqOjvbbl5iYGJphLiI+Pt7a/7N0BudhF87DLpyHXcL5PKy+cCw5OVm9e/dWfX293/76+nqlpqaGaCoAAILD6khHRUVpzJgxKi0t9e3r6OhQaWmpXC5XCCcDAKD7Wf92d2FhofLz8zV27Fj9zd/8jZYvX65Tp075rvYOF9HR0SouLr7grfhww3nYhfOwC+dhl55wHg5jjAn1EH/NT3/6U/34xz9WXV2dbrrpJq1YsUI5OTmhHgsAgG4VFpEGAOBKZPVn0gAAXMmINAAAliLSAABYikgDAGApIh1AK1eu1MiRI32/3cblcmnbtm2+42fOnFFBQYGSkpIUFxenvLy8C35Ri42WLFkih8OhuXPn+vaFw7k89dRTcjgcfltWVpbveDicwzlHjhzRjBkzlJSUpNjYWN14443as2eP77gxRk8++aTS0tIUGxur3NxcHThwIIQTXygjI+OC58PhcKigoEBS+Dwf7e3tWrBggTIzMxUbG6shQ4boBz/4gc6/Bjccng9Jam5u1ty5czVo0CDFxsZq/Pjx2r17t++4jefx5ptv6p577tHAgQPlcDi0efNmv+OXM/OJEyc0ffp0xcfHKzExUbNmzQrYl3sE3Bf8DgycZ8uWLebXv/61+fDDD01NTY35/ve/byIjI01VVZUxxpjvfe97Jj093ZSWlpo9e/aYcePGmfHjx4d46s/39ttvm4yMDDNy5EgzZ84c3/5wOJfi4mIzYsQIU1tb69v+/Oc/+46HwzkYY8yJEyfMoEGDzAMPPGAqKirMRx99ZHbs2GEOHjzoe8ySJUtMQkKC2bx5s3nvvffMV7/6VZOZmWlaWlpCOLm/Y8eO+T0Xr776qpFkXn/9dWNM+DwfTz/9tElKSjKvvPKKOXTokNm4caOJi4szzz77rO8x4fB8GGPM17/+dTN8+HBTVlZmDhw4YIqLi018fLz505/+ZIyx8zx+85vfmMcff9y8/PLLRtIFX8B0OTPfeeedZtSoUWbXrl3md7/7nbnuuuvMtGnTgnwml4dId7O+ffua//zP/zSNjY0mMjLSbNy40XesurraSDLl5eUhnPDSmpubzdChQ82rr75q/u7v/s4X6XA5l+LiYjNq1KiLHguXczDGmMcee8zcdtttlzze0dFhUlNTzY9//GPfvsbGRhMdHW1+8YtfBGPELpkzZ44ZMmSI6ejoCKvn46677jIzZ8702zd16lQzffp0Y0z4PB+nT582vXv3Nq+88orf/ptvvtk8/vjjYXEefxnpy5n5gw8+MJLM7t27fY/Ztm2bcTgc5siRI0Gb/XLxdnc3aW9v14YNG3Tq1Cm5XC5VVlaqra1Nubm5vsdkZWXJ6XSqvLw8hJNeWkFBge666y6/mSWF1bkcOHBAAwcO1ODBgzV9+nS53f//u2vD5Ry2bNmisWPH6mtf+5oGDBig0aNH6/nnn/cdP3TokOrq6vzOJSEhQTk5Odadyzmtra36+c9/rpkzZ8rhcITV8zF+/HiVlpbqww8/lCS99957euuttzR58mRJ4fN8nD17Vu3t7YqJifHbHxsbq7feeitszuN8lzNzeXm5EhMTNXbsWN9jcnNz1atXL1VUVAR95r/G+l8LGm727dsnl8ulM2fOKC4uTps2bdLw4cO1d+9eRUVFXfCNXCkpKaqrqwvNsJ9jw4YNeuedd/w+nzqnrq4uLM4lJydH69at07Bhw1RbW6uFCxfqS1/6kqqqqsLmHCTpo48+0sqVK1VYWKjvf//72r17tx555BFFRUUpPz/fN+/FvtLVtnM5Z/PmzWpsbNQDDzwgKXz+PyVJ8+fPl8fjUVZWlnr37q329nY9/fTTmj59uiSFzfPRp08fuVwu/eAHP1B2drZSUlL0i1/8QuXl5bruuuvC5jzOdzkz19XVacCAAX7HIyIi1K9fPyvPi0gH2LBhw7R37141NTXpf/7nf5Sfn6+ysrJQj9Uphw8f1pw5c/Tqq69e8F/Z4eTcKxtJGjlypHJycjRo0CC99NJLio2NDeFkndPR0aGxY8fqmWeekSSNHj1aVVVVWrVqlfLz80M8XdesWbNGkydP1sCBA0M9Sqe99NJLevHFF7V+/XqNGDFCe/fu1dy5czVw4MCwez7++7//WzNnztQ111yj3r176+abb9a0adNUWVkZ6tHwf3i7O8CioqJ03XXXacyYMVq8eLFGjRqlZ599VqmpqWptbVVjY6Pf42382s3KykodO3ZMN998syIiIhQREaGysjKtWLFCERERSklJCZtzOV9iYqKuv/56HTx4MKyej7S0NA0fPtxvX3Z2tu+t+3PzhstXun7yySd67bXX9O1vf9u3L5yej3/913/V/Pnzdf/99+vGG2/UN7/5Tc2bN0+LFy+WFF7Px5AhQ1RWVqaTJ0/q8OHDevvtt9XW1qbBgweH1Xmcczkzp6am6tixY37Hz549qxMnTlh5XkS6m3V0dMjr9WrMmDGKjIz0+9rNmpoaud1u6752c+LEidq3b5/27t3r28aOHavp06f7/jlczuV8J0+e1B//+EelpaWF1fNx6623qqamxm/fhx9+qEGDBkmSMjMzlZqa6ncuHo9HFRUV1p2LJK1du1YDBgzQXXfd5dsXTs/H6dOn1auX/786e/furY6ODknh93xI0tVXX620tDR9+umn2rFjh6ZMmRKW53E5M7tcLjU2Nvq9W7Bz5051dHTY+cVNob5yrSeZP3++KSsrM4cOHTLvv/++mT9/vnE4HOa3v/2tMeazW0ycTqfZuXOn2bNnj3G5XMblcoV46stz/tXdxoTHufzzP/+zeeONN8yhQ4fM//7v/5rc3FyTnJxsjh07ZowJj3Mw5rPb4CIiIszTTz9tDhw4YF588UVz1VVXmZ///Oe+xyxZssQkJiaaX/3qV+b99983U6ZMCfmtMhfT3t5unE6neeyxxy44Fi7PR35+vrnmmmt8t2C9/PLLJjk52Tz66KO+x4TL87F9+3azbds289FHH5nf/va3ZtSoUSYnJ8e0trYaY+w8j+bmZvPuu++ad99910gyS5cuNe+++6755JNPLnvmO++804wePdpUVFSYt956ywwdOpRbsK4EM2fONIMGDTJRUVGmf//+ZuLEib5AG2NMS0uL+ad/+ifTt29fc9VVV5l/+Id/MLW1tSGc+PL9ZaTD4Vzuu+8+k5aWZqKiosw111xj7rvvPr97i8PhHM7ZunWrueGGG0x0dLTJysoyq1ev9jve0dFhFixYYFJSUkx0dLSZOHGiqampCdG0l7Zjxw4j6aKzhcvz4fF4zJw5c4zT6TQxMTFm8ODB5vHHHzder9f3mHB5Pn75y1+awYMHm6ioKJOammoKCgpMY2Oj77iN5/H6668bSRds+fn5lz3z8ePHzbRp00xcXJyJj483Dz74oGlubg7B2fx1fFUlAACW4jNpAAAsRaQBALAUkQYAwFJEGgAASxFpAAAsRaQBALAUkQYAwFJEGgAASxFpAAAsRaQBALAUkQYAwFL/DwBwkpi3EC8eAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# how long is our training data?\n",
    "doc_lengths = []\n",
    "for triplet in triplets:\n",
    "    tokens = nltk.word_tokenize(triplet)\n",
    "    doc_lengths.append(len(tokens))\n",
    "doc_lengths = np.asarray(doc_lengths)\n",
    "sns.displot(doc_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7be28cbd-b469-486c-94a1-1b8fb23540cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47.57807308970099"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(doc_lengths)\n",
    "# on average, we have ~47.5 tokens per entry, a good thing for GPT2 embedding size of 768 in gpt-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35ea297d-9ed3-4444-834f-e0aee3340bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = config_dict['model_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04be7ade-4c30-415d-b9c9-c4efe4dc6b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# load gpt-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name, bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2bdd305-2a1e-43bf-85f4-c1379d111765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max model length is 1024 for this model, although the actual embedding size for GPT small is 768\n",
      "The beginning of sequence token <|startoftext|> token has the id 50257\n",
      "The end of sequence token <|endoftext|> has the id 50256\n",
      "The padding token <|pad|> has the id 50258\n"
     ]
    }
   ],
   "source": [
    "print(\"The max model length is {} for this model, although the actual embedding size for GPT small is 768\".format(tokenizer.model_max_length))\n",
    "print(\"The beginning of sequence token {} token has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.bos_token_id), tokenizer.bos_token_id))\n",
    "print(\"The end of sequence token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.eos_token_id), tokenizer.eos_token_id))\n",
    "print(\"The padding token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.pad_token_id), tokenizer.pad_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab14844a-0bee-425d-9036-8b9e93c43f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config to instantiate model\n",
    "configuration = GPT2Config.from_pretrained(model_name, output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f2b34ca-76bd-4c8d-a2fa-820003e6efba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name, config=configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ffedcee-b6c7-43d8-b622-68527843efb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50259, 768)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resize token embeddings for our custom tokens (e.g. bos_token)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9dd3b69-eaea-4e7e-8de3-1d58a81ec9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ae74060-8628-4706-b597-14b2d8597a31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50259, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50259, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sends model to current device - in this case CUDA\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5dc18a1-03e3-4c02-8d84-6e05af269bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed value\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdba5811-ed9e-4393-94d7-94ee67da836f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    return str(datetime.timedelta(seconds=int(round((elapsed)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53c41e7c-1f89-4bd7-a0d8-88f53e7c87a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random',\n",
    "    'name': 'gpt2 random sweep - with validation',\n",
    "    'metric': {'goal': 'minimize', 'name': 'val_loss'},\n",
    "    \n",
    "    # parameters is the nested dictionary of hyperparameters we are sweeping\n",
    "    'parameters': \n",
    "    {\n",
    "        'batch_size': {'values': [1, 2, 3]},\n",
    "        'epochs': {'values': [5, 10, 15]},\n",
    "        'learning_rate': {'max': 0.1, 'min': 0.0001},\n",
    "        'epsilon': {'values': [1e-7]},\n",
    "        'warmup_steps': {'values': [1e2]}\n",
    "     }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3cc521f-693e-452a-84ed-0cff6f0ee7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: tclin8lb\n",
      "Sweep URL: https://wandb.ai/andrewbrown/gpt2-sweep-reflector/sweeps/tclin8lb\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"gpt2-sweep-reflector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ccd88d87-d526-4edc-bb72-dafdf4811c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Dataset(Dataset):\n",
    "    # Inherits Dataset from PyTorch, a data primitive which\n",
    "    # stores samples and corresponding labels\n",
    "    # custom Dataset needs init, len, and getitem\n",
    "    # init runs once when instantiating Dataset object\n",
    "    def __init__(self, txt_list, tokenzier, gpt2_type='gpt2', max_length=768):\n",
    "        self.tokenizer = tokenizer,\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "        \n",
    "        # for each text list, encode it using tokenizer then unpacl encodings dict into:\n",
    "        # input_ids: numerical representations of our tokens\n",
    "        # attn_masks: indicates which tokens should be attended to (and which are pads)\n",
    "        for txt in txt_list:\n",
    "            # tokenize the txt with a custom start and end token\n",
    "            # encodings dict contains both our token input ids and attention mask\n",
    "            # truncation will clip sentences that are too long\n",
    "            # padding adds pad tokens until we reach max input sentence length 768\n",
    "            encodings_dict = tokenizer('<|startoftext|>'+ txt + '<|endoftext|>', truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "            \n",
    "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "    # overrides len() to returns the number of samples in our dataset\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    # loads and returns a sample from dataset at given index idx\n",
    "    # sometimes we need to do type swapping in getitem, but not here\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attn_masks[idx]\n",
    "            \n",
    "def build_dataset(triplets, tokenizer, batch_size, max_length=768, mode=\"random\"):\n",
    "    dataset = GPT2Dataset(triplets, tokenizer, max_length=768)\n",
    "    if mode == \"sample\":\n",
    "        return DataLoader(dataset, sampler=SequentialSampler(dataset), batch_size=batch_size)\n",
    "    else:\n",
    "        return DataLoader(dataset, sampler=RandomSampler(dataset), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "480425a6-7850-460f-b6e6-3f8ea1d06716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_dataloader, optimizer, scheduler):\n",
    "    training_stats = []\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    \"\"\"\n",
    "    Training for one epoch\n",
    "    \"\"\"\n",
    "    wandb.watch(model)\n",
    "    print(\"---Training Epoch...---\")\n",
    "\n",
    "    # start epoch timer\n",
    "    t0 = time.time()\n",
    "\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # sets model into train mode, not actual backprop\n",
    "    # dropout and batchnorm behave differently\n",
    "    # opposite of model.eval() for inference mode\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # grab input tokens, labels, and masks\n",
    "        input_tokens = batch[0].to(device)\n",
    "        # in this case, we're generating text,\n",
    "        # so label tokens are the input tokens shifted\n",
    "        label_tokens = batch[0].to(device)\n",
    "        attn_masks = batch[1].to(device)\n",
    "\n",
    "        # clear any gradients from model tensors\n",
    "        # prevents any gradient accumulation\n",
    "        model.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(input_tokens,\n",
    "                        labels=label_tokens,\n",
    "                        attention_mask=attn_masks,\n",
    "                        token_type_ids=None\n",
    "                       )\n",
    "\n",
    "        # grab loss from outputs\n",
    "        loss = outputs[0]\n",
    "\n",
    "        batch_loss = loss.item()  # detach from device with item\n",
    "        total_train_loss += batch_loss\n",
    "\n",
    "        # backpropagation step\n",
    "        # computes dloss/dx for every parameter x which has requires_grad=True.\n",
    "        # updates gradient values\n",
    "        # x.grad += dloss/dx\n",
    "        loss.backward()\n",
    "        \n",
    "        # step scheduler\n",
    "        # tells scheduler to increase learning rate\n",
    "        # using our warmup steps\n",
    "        scheduler.step()\n",
    "\n",
    "        # step optimizer\n",
    "        # updates the value of x using the gradient x.grad\n",
    "        # x += -lr * x.grad\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "    print(\"---Done Training Epoch!---\")\n",
    "    # measure how long the epoch took\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(f\"---Training epoch took {training_time} ---\")\n",
    "    # calculate average loss over all batches\n",
    "    return total_train_loss / len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb7ae0d5-cb1e-4eb6-8581-15604e39fc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(model, validation_dataloader):\n",
    "    \"\"\"\n",
    "    Validation\n",
    "    \"\"\"\n",
    "    print(\"---Running Validation...---\")\n",
    "    \n",
    "    # start batch timer\n",
    "    t0 = time.time()\n",
    "    \n",
    "    # set model to inference mode\n",
    "    model.eval()\n",
    "    \n",
    "    total_eval_loss = 0\n",
    "    \n",
    "    # evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        # grab input tokens, labels, and masks\n",
    "        input_tokens = batch[0].to(device)\n",
    "        # in this case, we're generating text,\n",
    "        # so label tokens are the input tokens shifted\n",
    "        label_tokens = batch[0].to(device)\n",
    "        attn_masks = batch[1].to(device)\n",
    "        \n",
    "        # freeze gradients\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_tokens,\n",
    "                            attention_mask=attn_masks,\n",
    "                            labels=label_tokens)\n",
    "            \n",
    "            loss = outputs[0]\n",
    "        \n",
    "        batch_loss = loss.item()\n",
    "        total_eval_loss += batch_loss\n",
    "    \n",
    "    # measure how long the epoch took\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"---Done Validation!---\")\n",
    "    return total_eval_loss / len(validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53d1f543-ed25-4df4-9988-a387ff05cd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch)\n",
    "def build_optimizer(model, learning_rate, epsilon):\n",
    "    return AdamW(model.parameters(), lr = learning_rate, eps = epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb55cfd4-6fab-4bac-9ea0-0693fcdb6416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_learning_rate_scheduler(optimizer, warmup_steps, num_examples, epochs):\n",
    "    # Create learning rate scheduler\n",
    "    # we schedule learning rate using optimzer, num_warmup steps, and num_training steps\n",
    "    total_steps = num_examples * epochs\n",
    "    return get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8643bc2d-72cc-4ec3-ad7b-be249af0bc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(project=\"gpt2-sweep-reflector\", notes=\"Trying to sweep gpt2 reflector\",tags=[\"gpt2\", \"reflector\", \"sweep\"]):\n",
    "        # If called by wandb.agent, as below,\n",
    "        print(\"starting sweep\")\n",
    "        # this config will be set by Sweep Controller\n",
    "        batch_size = wandb.config.batch_size\n",
    "        learning_rate = wandb.config.learning_rate\n",
    "        epsilon = wandb.config.epsilon\n",
    "        epochs = wandb.config.epochs\n",
    "        warmup_steps = wandb.config.warmup_steps\n",
    "\n",
    "        print(\"loaded config sweep\")\n",
    "\n",
    "        train_dataloader = build_dataset(triplets, tokenizer, batch_size, max_length=768, mode=\"random\")\n",
    "        validation_dataloader = build_dataset(val_triplets, tokenizer, batch_size, max_length=768, mode=\"sequential\")\n",
    "        # not varying model architecture\n",
    "        #network = build_network(config.fc_layer_size, config.dropout)\n",
    "        optimizer = build_optimizer(model, learning_rate, epsilon)\n",
    "        scheduler = build_learning_rate_scheduler(optimizer, warmup_steps, len(train_dataloader), epochs)\n",
    "        print(\"Starting training\")\n",
    "\n",
    "            \n",
    "        for epoch in range(epochs):\n",
    "            train_loss = train_epoch(model, train_dataloader, optimizer, scheduler)\n",
    "            val_loss = validate_epoch(model, validation_dataloader)\n",
    "            wandb.log({\n",
    "                'epoch': epoch, \n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss\n",
    "            })\n",
    "        wandb.alert(\n",
    "            title=\"Done Sweep! -  Feb 21\", \n",
    "            text=f\"Finished sweeping gpt-2, check out validation results\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36bfbbf-4cf5-4df5-8e38-3f0c7380b5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: d6rzao0j with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepsilon: 1e-07\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.09837234475193338\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mandrewbrown\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "time=\"2023-02-22T02:01:10Z\" level=fatal msg=\"Failed to discover neuron devices\" error=\"Unable to read device information from the driver. Make sure aws-neuron-dkms is installed and the neuron driver is accessible.\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/transformer-fine-tune/notebooks/wandb/run-20230222_020109-d6rzao0j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/andrewbrown/gpt2-sweep-reflector/runs/d6rzao0j\" target=\"_blank\">eternal-sweep-1</a></strong> to <a href=\"https://wandb.ai/andrewbrown/gpt2-sweep-reflector\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page: <a href=\"https://wandb.ai/andrewbrown/gpt2-sweep-reflector/sweeps/tclin8lb\" target=\"_blank\">https://wandb.ai/andrewbrown/gpt2-sweep-reflector/sweeps/tclin8lb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/andrewbrown/gpt2-sweep-reflector\" target=\"_blank\">https://wandb.ai/andrewbrown/gpt2-sweep-reflector</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href=\"https://wandb.ai/andrewbrown/gpt2-sweep-reflector/sweeps/tclin8lb\" target=\"_blank\">https://wandb.ai/andrewbrown/gpt2-sweep-reflector/sweeps/tclin8lb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/andrewbrown/gpt2-sweep-reflector/runs/d6rzao0j\" target=\"_blank\">https://wandb.ai/andrewbrown/gpt2-sweep-reflector/runs/d6rzao0j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting sweep\n",
      "loaded config sweep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/torch_p37/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "---Training Epoch...---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/torch_p37/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Done Training Epoch!---\n",
      "---Training epoch took 0:01:09 ---\n",
      "---Running Validation...---\n",
      "---Done Validation!---\n",
      "---Training Epoch...---\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, train, count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c5689d6a-934e-440e-906d-4325212bec40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ../../Output/model/gpt-feb5/\n"
     ]
    }
   ],
   "source": [
    "# saving and loading the finetuned model\n",
    "output_dir = '../../Output/model/gpt-feb6/'\n",
    "tokenizer_dir = '../../Output/model/gpt-tokenizer-feb6/'\n",
    "\n",
    "# create output directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# create output directory if needed\n",
    "if not os.path.exists(tokenizer_dir):\n",
    "    os.makedirs(tokenizer_dir)\n",
    "    \n",
    "    \n",
    "print(f\"Saving model to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ded92b74-16ff-4894-8fb4-a1fa20e7e76d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../../Output/model/gpt-tokenizer-feb5/tokenizer_config.json',\n",
       " '../../Output/model/gpt-tokenizer-feb5/special_tokens_map.json',\n",
       " '../../Output/model/gpt-tokenizer-feb5/vocab.json',\n",
       " '../../Output/model/gpt-tokenizer-feb5/merges.txt',\n",
       " '../../Output/model/gpt-tokenizer-feb5/added_tokens.json')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save trained model, configuration and tokenizer\n",
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(tokenizer_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "81249326-1640-4cb3-879a-95d16dbc8c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save training arguments with trained model\n",
    "torch.save(model.state_dict(), os.path.join(output_dir, 'training_args.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb50428-5226-4a66-95b1-9bce62d8f18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load a trained model and vocabulary that you have fine-tuned\n",
    "model = GPT2LMHeadModel.from_pretrained(output_dir)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(output_dir)\n",
    "model.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
