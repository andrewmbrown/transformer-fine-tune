model_name: gpt2
data_train_path: /home/ubuntu/transformer-fine-tune/data/instruct-reflections/train-delta.csv
data_validation_path: /home/ubuntu/transformer-fine-tune/data/instruct-reflections/validation-delta.csv
output_data_dir: /home/ubuntu/transformer-fine-tune/models/gpt2-small-delta
output_model_dir: /home/ubuntu/transformer-fine-tune/models/gpt2-small-delta
output_tokenizer_dir: /home/ubuntu/transformer-fine-tune/models/gpt2-small-delta
experiment_name: delta
wandb_project_name: gpt2-small-reflector-delta
wandb_notes: A small gpt-2 reflector trained on delta data (separate actual reflection part with a pound sign)
wandb_tags: GPT2, reflector, instruct, delta
hyperparameters:
  find_hyperparams_automatically: false
  num_trials: 10
  fp16: true
  deepspeed: true
  grad_accumulation_steps: 2
  eval_batch_size: 1
  learning_rate: 0.0003
  epochs: 10
  warmup_steps: 100
  epsilon: 1e-7
  batch_size: 1
  sample_every: 100
  seed: 42
  eval_steps: 10
  weight_decay: 0.01
refgen:
  temperature: 0.6
  repetition_penalty: 1.1
  do_sample: true
  top_k: 100
  top_p: 1
