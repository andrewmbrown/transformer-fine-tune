model_name: gpt2
pretrained: /
data_train_path: ../data/reflection-triplets/complete_reflection_triplets.csv
data_validation_path: ../data/reflection-triplets/complete_validation_reflection_triplets.csv
output_data_dir: ../models
output_model_dir: ../models/gpt2-xl-mi-reflector
output_tokenizer_dir: ../models/gpt2-xl-mi-reflector
output_training_args_dir: ../models/gpt2-xl-mi-reflector/training-args
path_to_deepspeed_config: ../configs/ds_config_zero2.json

hyperparameters:
  find_hyperparams_automatically: false
  num_trials: 10
  fp16: true
  deepspeed: true
  grad_accumulation_steps: 2
  eval_batch_size: 1
  learning_rate: 0.0003
  epochs: 1
  warmup_steps: 100
  epsilon: 1e-7
  batch_size: 1
  sample_every: 100
  seed: 42
  eval_steps: 10
  weight_decay: 0.01

wandb_project_name: gpt2-xl-reflector
wandb_notes: Using DeepSpeed to train gpt2-xl
wandb_tags: [
  GPT2Family, gpt2-xl, reflector, DeepSpeed, Multi-GPU
]

output_directory: ../../Output/test_results/
refgen:
  temperature: 0.6
  repetition_penalty: 1.1
  do_sample: true
  top_k: 100
  top_p: 1
